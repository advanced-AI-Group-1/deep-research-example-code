import os
from dotenv import load_dotenv

# .env íŒŒì¼ ë¡œë“œ
load_dotenv()


import uuid
import asyncio
from typing import TypedDict, List, Annotated, Optional
from dataclasses import dataclass
from IPython.display import display, Markdown
import operator

from langchain.chat_models import init_chat_model
from langchain_core.messages import HumanMessage, SystemMessage
from langgraph.graph import StateGraph, START, END
from langgraph.checkpoint.memory import MemorySaver
from pydantic import BaseModel


# 1. ë°ì´í„° ëª¨ë¸ ì •ì˜
@dataclass
class Section:
    name: str
    description: str
    research_needed: bool
    content: str = ""


class SearchQuery(BaseModel):
    query: str


class SearchQueries(BaseModel):
    queries: List[SearchQuery]


class Sections(BaseModel):
    sections: List[Section]


# 2. ë‹¨ìˆœí™”ëœ ìƒíƒœ ì •ì˜ (ë³‘ë ¬ ì²˜ë¦¬ ë¬¸ì œ í•´ê²°)
class ResearchState(TypedDict):
    topic: str
    sections: List[Section]
    current_section_index: int
    all_research_done: bool
    final_report: str


# 3. í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿
SECTION_PLANNER_PROMPT = """
ì£¼ì œ: {topic}

ìœ„ ì£¼ì œì— ëŒ€í•œ ì¢…í•©ì ì¸ ë³´ê³ ì„œë¥¼ ì‘ì„±í•˜ê¸° ìœ„í•´ ì„¹ì…˜ì„ ê³„íší•´ì£¼ì„¸ìš”.

ë‹¤ìŒ êµ¬ì¡°ë¥¼ ë”°ë¥´ì„¸ìš”:
1. ì†Œê°œ (ì—°êµ¬ ë¶ˆí•„ìš”)
2. ì£¼ìš” ë³¸ë¬¸ ì„¹ì…˜ë“¤ (ì—°êµ¬ í•„ìš”)
3. ê²°ë¡  (ì—°êµ¬ ë¶ˆí•„ìš”)

ê° ì„¹ì…˜ì€ ë‹¤ìŒì„ í¬í•¨í•´ì•¼ í•©ë‹ˆë‹¤:
- name: ì„¹ì…˜ ì œëª©
- description: ì„¹ì…˜ ì„¤ëª… (ë¬´ì—‡ì„ ë‹¤ë£°ì§€)
- research_needed: ì›¹ ê²€ìƒ‰ì´ í•„ìš”í•œì§€ ì—¬ë¶€

5-6ê°œ ì„¹ì…˜ìœ¼ë¡œ êµ¬ì„±í•´ì£¼ì„¸ìš”.
"""

QUERY_GENERATOR_PROMPT = """
ì£¼ì œ: {topic}
ì„¹ì…˜: {section_name}
ì„¹ì…˜ ì„¤ëª…: {section_description}

ì´ ì„¹ì…˜ì„ ì‘ì„±í•˜ê¸° ìœ„í•´ í•„ìš”í•œ ì›¹ ê²€ìƒ‰ ì¿¼ë¦¬ 3ê°œë¥¼ ìƒì„±í•´ì£¼ì„¸ìš”.
ê° ì¿¼ë¦¬ëŠ” êµ¬ì²´ì ì´ê³  ê²€ìƒ‰ ê°€ëŠ¥í•œ í˜•íƒœì—¬ì•¼ í•©ë‹ˆë‹¤.
"""

SECTION_WRITER_PROMPT = """
ì£¼ì œ: {topic}
ì„¹ì…˜ ì œëª©: {section_name}
ì„¹ì…˜ ì„¤ëª…: {section_description}

ë‹¤ìŒ ê²€ìƒ‰ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ í•´ë‹¹ ì„¹ì…˜ì„ ì‘ì„±í•´ì£¼ì„¸ìš”:

ê²€ìƒ‰ ê²°ê³¼:
{search_results}

ìš”êµ¬ì‚¬í•­:
- ë§ˆí¬ë‹¤ìš´ í˜•ì‹ìœ¼ë¡œ ì‘ì„±
- êµ¬ì²´ì ì´ê³  ìƒì„¸í•œ ë‚´ìš©
- ê²€ìƒ‰ ê²°ê³¼ì˜ ì •ë³´ë¥¼ ì¢…í•©í•˜ì—¬ ì‘ì„±
- ì„¹ì…˜ ì œëª© í¬í•¨ (## {section_name})
"""


# 4. ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜
async def search_web_tavily(queries: List[str]) -> str:
    """Tavily APIë¥¼ ì‚¬ìš©í•œ ì›¹ ê²€ìƒ‰"""
    import os
    try:
        from tavily import TavilyClient
        
        api_key = os.getenv("TAVILY_API_KEY")
        if not api_key:
            return "Tavily API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."
        
        client = TavilyClient(api_key=api_key)
        
        all_results = []
        for query in queries:
            try:
                response = client.search(query=query, max_results=3)
                results = response.get('results', [])
                
                for result in results:
                    content = f"ì œëª©: {result.get('title', '')}\n"
                    content += f"ë‚´ìš©: {result.get('content', '')}\n"
                    content += f"URL: {result.get('url', '')}\n"
                    all_results.append(content)
            except Exception as e:
                print(f"ê²€ìƒ‰ ì˜¤ë¥˜ ({query}): {e}")
                continue
        
        return "\n\n---\n\n".join(all_results)
    
    except ImportError:
        return f"ê²€ìƒ‰ ëª¨ì˜ ê²°ê³¼: {', '.join(queries)}ì— ëŒ€í•œ ê²€ìƒ‰ ê²°ê³¼ì…ë‹ˆë‹¤."


def save_markdown_to_file(content: str, topic: str, output_dir: str = "reports"):
    """ë§ˆí¬ë‹¤ìš´ ë‚´ìš©ì„ íŒŒì¼ë¡œ ì €ì¥"""
    from datetime import datetime
    
    # ì¶œë ¥ ë””ë ‰í† ë¦¬ ìƒì„±
    os.makedirs(output_dir, exist_ok=True)
    
    # íŒŒì¼ëª… ìƒì„± (ì•ˆì „í•œ íŒŒì¼ëª…ìœ¼ë¡œ ë³€í™˜)
    safe_topic = "".join(c for c in topic if c.isalnum() or c in (' ', '-', '_')).rstrip()
    safe_topic = safe_topic.replace(' ', '_')[:50]  # ìµœëŒ€ 50ìë¡œ ì œí•œ
    
    # íƒ€ì„ìŠ¤íƒ¬í”„ ì¶”ê°€
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    filename = f"{safe_topic}_{timestamp}.md"
    filepath = os.path.join(output_dir, filename)
    
    # íŒŒì¼ ì €ì¥
    try:
        with open(filepath, 'w', encoding='utf-8') as f:
            f.write(content)
        
        print(f"ğŸ“ ë³´ê³ ì„œê°€ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤: {filepath}")
        return filepath
        
    except Exception as e:
        print(f"âŒ íŒŒì¼ ì €ì¥ ì‹¤íŒ¨: {e}")
        return None


# 5. ë…¸ë“œ í•¨ìˆ˜ë“¤ (ìˆœì°¨ ì²˜ë¦¬ ë°©ì‹)
async def plan_sections(state: ResearchState) -> dict:
    """ë³´ê³ ì„œ ì„¹ì…˜ ê³„íš ìƒì„±"""
    print("ğŸ“‹ ë³´ê³ ì„œ ì„¹ì…˜ ê³„íš ì¤‘...")
    
    # LLM ì´ˆê¸°í™”
    llm = init_chat_model(
        model="claude-3-5-haiku-latest",
        model_provider="anthropic"
    )
    
    structured_llm = llm.with_structured_output(Sections)
    
    # í”„ë¡¬í”„íŠ¸ ìƒì„±
    prompt = SECTION_PLANNER_PROMPT.format(topic=state["topic"])
    
    # ì„¹ì…˜ ìƒì„±
    response = await structured_llm.ainvoke([
        SystemMessage(content=prompt),
        HumanMessage(content="ë³´ê³ ì„œ ì„¹ì…˜ì„ ê³„íší•´ì£¼ì„¸ìš”.")
    ])
    
    sections = response.sections
    print(f"âœ… {len(sections)}ê°œ ì„¹ì…˜ ê³„íš ì™„ë£Œ")
    
    return {
        "sections": sections,
        "current_section_index": 0,
        "all_research_done": False
    }


async def process_research_sections(state: ResearchState) -> dict:
    """ì—°êµ¬ê°€ í•„ìš”í•œ ëª¨ë“  ì„¹ì…˜ì„ ìˆœì°¨ì ìœ¼ë¡œ ì²˜ë¦¬"""
    print("ğŸ”¬ ì—°êµ¬ ì„¹ì…˜ë“¤ ì²˜ë¦¬ ì‹œì‘...")
    
    sections = state["sections"]
    research_sections = [s for s in sections if s.research_needed]
    
    print(f"ğŸ“Š ì´ {len(research_sections)}ê°œ ì—°êµ¬ ì„¹ì…˜ ì²˜ë¦¬ ì˜ˆì •")
    
    # LLM ì´ˆê¸°í™” (í•œ ë²ˆë§Œ)
    query_llm = init_chat_model(
        model="claude-3-5-haiku-latest",
        model_provider="anthropic"
    ).with_structured_output(SearchQueries)
    
    writer_llm = init_chat_model(
        model="claude-3-5-haiku-latest",
        model_provider="anthropic"
    )
    
    # ê° ì—°êµ¬ ì„¹ì…˜ì„ ìˆœì°¨ì ìœ¼ë¡œ ì²˜ë¦¬
    for i, section in enumerate(research_sections, 1):
        print(f"\nğŸ” [{i}/{len(research_sections)}] '{section.name}' ì„¹ì…˜ ì²˜ë¦¬ ì¤‘...")
        
        try:
            # 1. ê²€ìƒ‰ ì¿¼ë¦¬ ìƒì„±
            query_prompt = QUERY_GENERATOR_PROMPT.format(
                topic=state["topic"],
                section_name=section.name,
                section_description=section.description
            )
            
            query_response = await query_llm.ainvoke([
                SystemMessage(content=query_prompt),
                HumanMessage(content="ê²€ìƒ‰ ì¿¼ë¦¬ë¥¼ ìƒì„±í•´ì£¼ì„¸ìš”.")
            ])
            
            queries = [q.query for q in query_response.queries]
            print(f"  ğŸ“ {len(queries)}ê°œ ê²€ìƒ‰ ì¿¼ë¦¬ ìƒì„± ì™„ë£Œ")
            
            # 2. ì›¹ ê²€ìƒ‰
            print(f"  ğŸŒ ì›¹ ê²€ìƒ‰ ì‹¤í–‰ ì¤‘...")
            search_results = await search_web_tavily(queries)
            print(f"  âœ… ê²€ìƒ‰ ì™„ë£Œ ({len(search_results)} ë¬¸ì)")
            
            # 3. ì„¹ì…˜ ì‘ì„±
            print(f"  âœï¸ ì„¹ì…˜ ë‚´ìš© ì‘ì„± ì¤‘...")
            writer_prompt = SECTION_WRITER_PROMPT.format(
                topic=state["topic"],
                section_name=section.name,
                section_description=section.description,
                search_results=search_results
            )
            
            writer_response = await writer_llm.ainvoke([
                SystemMessage(content=writer_prompt),
                HumanMessage(content="ì„¹ì…˜ì„ ì‘ì„±í•´ì£¼ì„¸ìš”.")
            ])
            
            # ì„¹ì…˜ ë‚´ìš© ì—…ë°ì´íŠ¸
            section.content = writer_response.content
            print(f"  âœ… '{section.name}' ì„¹ì…˜ ì™„ë£Œ!")
        
        except Exception as e:
            print(f"  âŒ '{section.name}' ì„¹ì…˜ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
            section.content = f"## {section.name}\n\n{section.description}\n\n(ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e})"
    
    print(f"\nğŸ‰ ëª¨ë“  ì—°êµ¬ ì„¹ì…˜ ì²˜ë¦¬ ì™„ë£Œ!")
    return {
        "sections": sections,
        "all_research_done": True
    }


async def write_non_research_sections(state: ResearchState) -> dict:
    """ì—°êµ¬ê°€ í•„ìš”í•˜ì§€ ì•Šì€ ì„¹ì…˜ë“¤ ì‘ì„±"""
    print("ğŸ“ ì†Œê°œ/ê²°ë¡  ì„¹ì…˜ ì‘ì„± ì¤‘...")
    
    sections = state["sections"]
    
    for section in sections:
        if not section.research_needed and not section.content:
            # ì—°êµ¬ê°€ í•„ìš”í•˜ì§€ ì•Šì€ ì„¹ì…˜ì€ ê°„ë‹¨íˆ ì‘ì„±
            if "ì†Œê°œ" in section.name.lower() or "introduction" in section.name.lower():
                content = f"""## {section.name}

{section.description}

ì´ ë³´ê³ ì„œëŠ” {state['topic']}ì— ëŒ€í•œ ì¢…í•©ì ì¸ ë¶„ì„ì„ ì œê³µí•©ë‹ˆë‹¤."""
            else:  # ê²°ë¡ 
                content = f"""## {section.name}

{section.description}

ì´ìƒìœ¼ë¡œ {state['topic']}ì— ëŒ€í•œ ë¶„ì„ì„ ë§ˆì¹©ë‹ˆë‹¤."""
            
            section.content = content
            print(f"âœ… '{section.name}' ì„¹ì…˜ ì‘ì„± ì™„ë£Œ")
    
    return {"sections": sections}


async def compile_final_report(state: ResearchState) -> dict:
    """ìµœì¢… ë³´ê³ ì„œ ì»´íŒŒì¼"""
    print("ğŸ“Š ìµœì¢… ë³´ê³ ì„œ ì»´íŒŒì¼ ì¤‘...")
    
    sections = state["sections"]
    
    # ì„¹ì…˜ ë‚´ìš© í•©ì¹˜ê¸°
    sections_content = []
    for section in sections:
        if section.content:
            sections_content.append(section.content)
        else:
            # ë¹ˆ ì„¹ì…˜ì´ ìˆë‹¤ë©´ ê¸°ë³¸ ë‚´ìš© ì¶”ê°€
            sections_content.append(f"## {section.name}\n\n{section.description}")
    
    # ìµœì¢… ë³´ê³ ì„œ ìƒì„±
    final_report = "\n\n".join(sections_content)
    
    print("âœ… ìµœì¢… ë³´ê³ ì„œ ì»´íŒŒì¼ ì™„ë£Œ")
    return {"final_report": final_report}


# 6. ì¡°ê±´ë¶€ ë¼ìš°íŒ… í•¨ìˆ˜
def should_continue_research(state: ResearchState) -> str:
    """ì—°êµ¬ê°€ ì™„ë£Œë˜ì—ˆëŠ”ì§€ í™•ì¸"""
    if state.get("all_research_done", False):
        return "write_non_research"
    else:
        return "process_research"


# 7. ë‹¨ìˆœí™”ëœ ê·¸ë˜í”„ ìƒì„±
def create_research_agent():
    """ë‹¨ìˆœí™”ëœ ìë™ ì—°êµ¬ ì—ì´ì „íŠ¸ ìƒì„±"""
    
    # ë©”ì¸ ê·¸ë˜í”„ (ìˆœì°¨ ì²˜ë¦¬)
    main_graph = StateGraph(ResearchState)
    
    main_graph.add_node("plan_sections", plan_sections)
    main_graph.add_node("process_research", process_research_sections)
    main_graph.add_node("write_non_research", write_non_research_sections)
    main_graph.add_node("compile_report", compile_final_report)
    
    # ì—£ì§€ ì—°ê²° (ìˆœì°¨ì )
    main_graph.add_edge(START, "plan_sections")
    main_graph.add_conditional_edges(
        "plan_sections",
        should_continue_research,
        {
            "process_research": "process_research",
            "write_non_research": "write_non_research"
        }
    )
    main_graph.add_edge("process_research", "write_non_research")
    main_graph.add_edge("write_non_research", "compile_report")
    main_graph.add_edge("compile_report", END)
    
    return main_graph.compile(checkpointer=MemorySaver())


def visualize_graph(show_xray=True):
    """ê·¸ë˜í”„ êµ¬ì¡° ì‹œê°í™”"""
    print("ğŸ“Š ê·¸ë˜í”„ êµ¬ì¡° ì‹œê°í™” ì¤‘...")
    
    try:
        # ê·¸ë˜í”„ ìƒì„±
        graph = create_research_agent()
        
        # ê·¸ë˜í”„ ì´ë¯¸ì§€ ìƒì„±
        if show_xray:
            # X-ray ëª¨ë“œ: ì„œë¸Œê·¸ë˜í”„ê¹Œì§€ ìì„¸íˆ ë³´ê¸°
            graph_image = graph.get_graph(xray=1).draw_mermaid_png()
        else:
            # ê¸°ë³¸ ëª¨ë“œ: ë©”ì¸ ë…¸ë“œë§Œ ë³´ê¸°
            graph_image = graph.get_graph().draw_mermaid_png()
        
        # ì´ë¯¸ì§€ í‘œì‹œ
        from IPython.display import Image, display
        display(Image(graph_image))
        
        print("âœ… ê·¸ë˜í”„ ì‹œê°í™” ì™„ë£Œ!")
        
        return graph_image
    
    except Exception as e:
        print(f"âŒ ê·¸ë˜í”„ ì‹œê°í™” ì‹¤íŒ¨: {e}")
        print("Mermaid ë˜ëŠ” graphviz íŒ¨í‚¤ì§€ê°€ í•„ìš”í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
        return None


# 8. ì‚¬ìš© í•¨ìˆ˜ (ìˆ˜ì •ëœ ë²„ì „)
async def run_research_agent(topic: str, display_result: bool = True, save_to_file: bool = True):
    """ì—°êµ¬ ì—ì´ì „íŠ¸ ì‹¤í–‰"""
    print(f"ğŸš€ ì—°êµ¬ ì‹œì‘: {topic}")
    print("=" * 80)
    
    # ê·¸ë˜í”„ ìƒì„±
    graph = create_research_agent()
    
    # ì‹¤í–‰ ì„¤ì •
    config = {"configurable": {"thread_id": str(uuid.uuid4())}}
    
    # ì´ˆê¸° ìƒíƒœ
    initial_state = {
        "topic": topic,
        "sections": [],
        "current_section_index": 0,
        "all_research_done": False,
        "final_report": ""
    }
    
    # ì‹¤í–‰
    try:
        final_state = await graph.ainvoke(initial_state, config)
        
        print("=" * 80)
        print("ğŸ‰ ì—°êµ¬ ì™„ë£Œ!")
        
        final_report = final_state.get("final_report", "")
        
        if display_result and final_report:
            print("\nğŸ“„ ìµœì¢… ë³´ê³ ì„œ:")
            print("-" * 40)
            # Jupyterê°€ ì•„ë‹Œ í™˜ê²½ì—ì„œëŠ” ë§ˆí¬ë‹¤ìš´ ë‚´ìš©ì„ ì§ì ‘ ì¶œë ¥
            print(final_report)
        
        # íŒŒì¼ ì €ì¥
        if save_to_file and final_report:
            save_markdown_to_file(final_report, topic)
        
        return final_report
        
    except Exception as e:
        print(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
        import traceback
        traceback.print_exc()
        return None


# ê°„ë‹¨í•œ ì‹¤í–‰ í•¨ìˆ˜ ì¶”ê°€
def run_research_sync(topic: str, save_file: bool = True, show_output: bool = True):
    """ë™ê¸°ì ìœ¼ë¡œ ì—°êµ¬ ì‹¤í–‰ (íŒŒì¼ ì €ì¥ í¬í•¨)"""
    return asyncio.run(run_research_agent(
        topic=topic, 
        display_result=show_output, 
        save_to_file=save_file
    ))


# ì¦‰ì‹œ ê·¸ë˜í”„ ì‹œê°í™” (ì„ íƒì‚¬í•­)
def show_graph_now():
    """ì§€ê¸ˆ ì¦‰ì‹œ ê·¸ë˜í”„ ë³´ê¸°"""
    print("ğŸ¨ ê·¸ë˜í”„ êµ¬ì¡° ë¯¸ë¦¬ë³´ê¸°:")
    return visualize_graph(show_xray=False)


# ê·¸ë˜í”„ ì‹œê°í™” ì‹¤í–‰
show_graph_now()

# ê·¸ë˜í”„ ìƒì„±
graph = create_research_agent()


# 9. ì‚¬ìš© ì˜ˆì œ
if __name__ == "__main__":
    print("ì‚¬ìš©ë²•:")
    print("# ê·¸ë˜í”„ êµ¬ì¡° ë³´ê¸°")
    print("visualize_graph()")
    print("\n# ì—°êµ¬ ì‹¤í–‰ (ë¹„ë™ê¸°)")
    print("await run_research_agent(topic)")
    print("\n# ì—°êµ¬ ì‹¤í–‰ (ë™ê¸°)")
    print("run_research_sync(topic)")
    print("\n# ìˆ˜ë™ ì‹¤í–‰")
    print("graph = create_research_agent()")
    print("result = await graph.ainvoke({'topic': topic, 'sections': [], 'current_section_index': 0, 'all_research_done': False, 'final_report': ''})")


if __name__ == "__main__":
    # ì‚¬ìš© ì˜ˆì œ
    topic_example = "Overview of Model Context Protocol (MCP), an Anthropicâ€‘backed open standard for integrating external context and tools with LLMs"
    
    # ë°©ë²• 1: ë¹„ë™ê¸° ì‹¤í–‰ (ê¸°ì¡´ ë°©ì‹)
    # asyncio.run(run_research_agent(topic_example))
    
    # ë°©ë²• 2: ë™ê¸° ì‹¤í–‰ (ìƒˆë¡œìš´ ë°©ì‹, ê¶Œì¥)
    run_research_sync(topic_example)